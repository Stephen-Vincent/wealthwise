# ai_models/stock_model/enhanced_stock_recommender.py

import os
import pandas as pd
import numpy as np
import yfinance as yf
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timedelta
import logging
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error
import joblib
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedStockRecommender:
    """
    Enhanced stock recommendation system with AI-powered optimization.
    
    AI Techniques Used:
    1. Machine Learning (Random Forest) for pattern recognition
    2. Market Regime Detection using time series analysis
    3. Factor Analysis for stock selection
    4. Correlation Analysis for portfolio optimization
    5. Statistical Learning for risk assessment
    6. Predictive Analytics for goal achievement
    
    Key Features:
    - Goal-oriented optimization
    - Market-adaptive recommendations
    - Risk-adjusted asset allocation
    - Multi-factor stock selection
    - Correlation-based diversification
    """
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self._last_recommendation_metadata = {}
        
        # Enhanced features including goal-achievement metrics
        self.feature_columns = ["target_value", "timeframe", "risk_score", "required_annual_return", "investment_gap"]
        
        # ASSET UNIVERSES: Organized by risk/return characteristics and asset classes
        self.asset_universes = {
            "ultra_conservative": {
                # Focus on capital preservation with modest growth
                "bonds_govt": ["TLT", "IEF", "SHY"],  # Government bonds (different durations)
                "bonds_corporate": ["LQD", "VCIT", "BND"],  # High-grade corporate bonds
                "dividend_aristocrats": ["NOBL", "VIG", "DVY"],  # Dividend growth stocks
                "utilities": ["VPU", "XLU"],  # Utility sector ETFs
                "allocation": {"bonds_govt": 0.4, "bonds_corporate": 0.3, "dividend_aristocrats": 0.2, "utilities": 0.1},
                "expected_annual_return": 0.05,  # 5% expected annual return
                "volatility": 0.08  # 8% annual volatility
            },
            "conservative": {
                # Balanced approach favoring stability with moderate growth
                "bonds": ["BND", "AGG", "VTEB"],  # Bond market exposure
                "large_cap_value": ["VTV", "VYM", "SCHV"],  # Value stocks with dividends
                "international_developed": ["VEA", "VXUS"],  # Developed market exposure
                "reits": ["VNQ", "SCHH"],  # Real estate exposure
                "allocation": {"bonds": 0.4, "large_cap_value": 0.35, "international_developed": 0.15, "reits": 0.1},
                "expected_annual_return": 0.07,  # 7% expected annual return
                "volatility": 0.12  # 12% annual volatility
            },
            "moderate": {
                # Balanced growth and income approach
                "large_cap_blend": ["VTI", "ITOT", "SWTSX"],  # Broad market exposure
                "international_blend": ["VEA", "VWO", "VTIAX"],  # Global diversification
                "bonds": ["BND", "AGG"],  # Bond foundation
                "sector_rotation": ["VGT", "VHT", "VFH"],  # Growth sectors
                "allocation": {"large_cap_blend": 0.4, "international_blend": 0.25, "bonds": 0.2, "sector_rotation": 0.15},
                "expected_annual_return": 0.09,  # 9% expected annual return
                "volatility": 0.15  # 15% annual volatility
            },
            "moderate_aggressive": {
                # Growth-focused with some stability
                "large_cap_growth": ["VUG", "MGK", "SCHG"],  # Growth stocks
                "small_cap": ["VB", "IWM", "VTI"],  # Small cap exposure
                "international_growth": ["VEA", "VWO", "IEMG"],  # International growth
                "tech_innovation": ["VGT", "ARKK", "QQQ"],  # Technology focus
                "allocation": {"large_cap_growth": 0.35, "small_cap": 0.25, "international_growth": 0.25, "tech_innovation": 0.15},
                "expected_annual_return": 0.11,  # 11% expected annual return
                "volatility": 0.18  # 18% annual volatility
            },
            "aggressive": {
                # Maximum growth potential with higher volatility
                "growth_stocks": ["VUG", "QQQ", "VGT"],  # High-growth stocks
                "small_cap_growth": ["VBK", "IWO", "VTWO"],  # Small cap growth
                "emerging_markets": ["VWO", "IEMG", "EEM"],  # Emerging market exposure
                "innovation_themes": ["ARKK", "ARKQ", "ARKG"],  # Thematic investing
                "allocation": {"growth_stocks": 0.4, "small_cap_growth": 0.25, "emerging_markets": 0.2, "innovation_themes": 0.15},
                "expected_annual_return": 0.13,  # 13% expected annual return
                "volatility": 0.22  # 22% annual volatility
            },
            "ultra_aggressive": {
                # Highest risk/reward seeking maximum growth
                "high_growth_tech": ["ARKK", "WCLD", "SKYY"],  # High-growth technology
                "crypto_exposure": ["BITO", "COIN"],  # Cryptocurrency exposure
                "biotech_innovation": ["XBI", "IBB", "ARKG"],  # Biotech sector
                "disruptive_tech": ["ARKQ", "ROBO", "FINX"],  # Disruptive technologies
                "allocation": {"high_growth_tech": 0.4, "crypto_exposure": 0.25, "biotech_innovation": 0.2, "disruptive_tech": 0.15},
                "expected_annual_return": 0.16,  # 16% expected annual return
                "volatility": 0.28  # 28% annual volatility
            }
        }
        
        # BACKUP TICKERS: Reliable ETFs for each risk category
        self.backup_tickers = {
            "ultra_conservative": ["BND", "VTI", "VEA", "VYM", "VTEB"],
            "conservative": ["VTI", "BND", "VEA", "VWO", "VYM"],
            "moderate": ["VTI", "VEA", "VWO", "BND", "VNQ"],
            "moderate_aggressive": ["VTI", "VUG", "VEA", "VWO", "QQQ"],
            "aggressive": ["QQQ", "VUG", "VWO", "ARKK", "VGT"],
            "ultra_aggressive": ["ARKK", "QQQ", "VUG", "VWO", "VGT"]
        }

    # =====================================================================
    # EXISTING METHODS (unchanged for backward compatibility)
    # =====================================================================
    
    def calculate_required_return(self, target_value: float, current_investment: float, 
                                timeframe: int, monthly_contribution: float = 0) -> float:
        """
        Calculate the required annual return to reach the target goal.
        
        This is KEY IMPROVEMENT #1: Goal-oriented optimization
        Instead of just picking stocks by risk, we calculate what return is needed
        to reach the user's goal and factor that into recommendations.
        """
        try:
            # Calculate total contributions over timeframe
            total_contributions = current_investment + (monthly_contribution * 12 * timeframe)
            
            if total_contributions <= 0:
                return 0.10  # Default 10% if no contributions
            
            # Calculate required compound annual growth rate (CAGR)
            # Formula: CAGR = (Ending Value / Beginning Value)^(1/years) - 1
            required_multiplier = target_value / total_contributions
            required_annual_return = (required_multiplier ** (1/timeframe)) - 1
            
            # Cap at reasonable bounds (0% to 25% annually)
            required_annual_return = max(0.0, min(0.25, required_annual_return))
            
            logger.info(f"Goal analysis: Need {required_annual_return:.1%} annual return to reach Â£{target_value:,.0f} target")
            return required_annual_return
            
        except Exception as e:
            logger.warning(f"Error calculating required return: {e}")
            return 0.10  # Default 10% annual return
    
    def assess_goal_feasibility(self, required_return: float, risk_score: float) -> Dict[str, any]:
        """
        Assess whether the goal is realistic given the user's risk tolerance.
        
        KEY IMPROVEMENT #2: Reality check for user expectations
        This helps set realistic expectations and suggests adjustments if needed.
        """
        risk_category = self.risk_score_to_category(risk_score)
        expected_return = self.asset_universes[risk_category]["expected_annual_return"]
        volatility = self.asset_universes[risk_category]["volatility"]
        
        # Calculate goal feasibility score (0-100%)
        if required_return <= expected_return:
            feasibility = min(100, 90 + (expected_return - required_return) * 100)
        else:
            # Goal requires higher returns than risk tolerance typically provides
            return_gap = required_return - expected_return
            feasibility = max(10, 70 - (return_gap * 200))
        
        assessment = {
            "feasibility_score": feasibility,
            "required_return": required_return,
            "expected_return": expected_return,
            "return_gap": required_return - expected_return,
            "risk_category": risk_category,
            "recommendation": self._get_feasibility_recommendation(feasibility, required_return, expected_return)
        }
        
        logger.info(f"Goal feasibility: {feasibility:.0f}% ({assessment['recommendation']})")
        return assessment
    
    def _get_feasibility_recommendation(self, feasibility: float, required: float, expected: float) -> str:
        """Generate recommendation based on goal feasibility analysis."""
        if feasibility >= 80:
            return "Highly achievable with current risk tolerance"
        elif feasibility >= 60:
            return "Achievable but may require market outperformance"
        elif feasibility >= 40:
            return "Challenging - consider increasing contributions or timeframe"
        else:
            return "Very challenging - recommend increasing risk tolerance, contributions, or timeframe"
    
    def risk_score_to_category(self, risk_score: float) -> str:
        """
        Convert numerical risk score to risk category.
        
        EXPLANATION: Maps 0-100 risk scores to investment categories
        Lower scores = more conservative, higher scores = more aggressive
        """
        if risk_score < 15:
            return "ultra_conservative"  # Capital preservation focus
        elif risk_score < 30:
            return "conservative"        # Income and modest growth
        elif risk_score < 50:
            return "moderate"           # Balanced growth and income
        elif risk_score < 70:
            return "moderate_aggressive" # Growth focus with some stability
        elif risk_score < 85:
            return "aggressive"         # High growth seeking
        else:
            return "ultra_aggressive"   # Maximum growth potential

    # =====================================================================
    # NEW AI-ENHANCED METHODS
    # =====================================================================

    def detect_market_regime(self) -> Dict[str, any]:
        """
        FINAL FIX: Market Regime Detection using Time Series Analysis
        """
        try:
            # Download recent market data
            spy = yf.download("SPY", period="1y", progress=False)['Close']
            vix = yf.download("^VIX", period="3mo", progress=False)['Close']
            
            if len(spy) < 50:
                logger.warning("Insufficient SPY data for regime detection")
                return self._default_market_regime()
            
            if len(vix) < 20:
                logger.warning("Insufficient VIX data for regime detection")
                return self._default_market_regime()
            
            # Calculate market indicators - ENSURE we get scalar values
            current_price = float(spy.iloc[-1])
            sma_20 = float(spy.rolling(20).mean().iloc[-1])
            sma_50 = float(spy.rolling(50).mean().iloc[-1]) if len(spy) >= 50 else current_price
            sma_200 = float(spy.rolling(200).mean().iloc[-1]) if len(spy) >= 200 else current_price
            
            # Trend analysis using multiple moving averages
            trend_score = 0
            if current_price > sma_20: 
                trend_score += 1
            if current_price > sma_50: 
                trend_score += 1
            if current_price > sma_200: 
                trend_score += 1
            if sma_20 > sma_50: 
                trend_score += 1
            if sma_50 > sma_200: 
                trend_score += 1
            
            # Volatility analysis - ENSURE scalar values
            current_vix = float(vix.iloc[-1])
            avg_vix = float(vix.mean())
            
            # Recent performance analysis - ENSURE scalar values
            returns_1m = 0.0
            returns_3m = 0.0
            
            if len(spy) >= 21:
                returns_1m = float((spy.iloc[-1] / spy.iloc[-21]) - 1)
            if len(spy) >= 63:
                returns_3m = float((spy.iloc[-1] / spy.iloc[-63]) - 1)
            
            # AI-based regime classification
            if trend_score >= 4 and current_vix < 25 and returns_3m > 0.05:
                regime = "strong_bull"
                confidence = 0.85
            elif trend_score >= 3 and returns_1m > 0:
                regime = "bull"
                confidence = 0.75
            elif trend_score <= 1 and returns_3m < -0.10:
                regime = "bear"
                confidence = 0.80
            elif current_vix > 30:
                regime = "high_volatility"
                confidence = 0.70
            elif current_vix < 15:
                regime = "low_volatility"
                confidence = 0.65
            else:
                regime = "sideways"
                confidence = 0.60
            
            logger.info(f"â Market regime: {regime} (VIX: {current_vix:.1f}, Trend: {trend_score}/5, Returns: 1M={returns_1m:.1%}, 3M={returns_3m:.1%})")
            
            return {
                "regime": regime,
                "confidence": confidence,
                "trend_score": trend_score,
                "current_vix": current_vix,
                "returns_1m": returns_1m,
                "returns_3m": returns_3m,
                "adjustment_factor": self._get_regime_adjustment(regime)
            }
            
        except Exception as e:
            logger.warning(f"Market regime detection failed: {e}")
            return self._default_market_regime()

    def _default_market_regime(self) -> Dict[str, any]:
        """Default market regime when detection fails."""
        return {
            "regime": "neutral",
            "confidence": 0.50,
            "trend_score": 2.5,
            "current_vix": 20,
            "returns_1m": 0,
            "returns_3m": 0,
            "adjustment_factor": {"growth_tilt": 0, "defensive_tilt": 0, "volatility_adjustment": 0}
        }

    def _get_regime_adjustment(self, regime: str) -> Dict[str, float]:
        """Get portfolio adjustments based on market regime."""
        adjustments = {
            "strong_bull": {"growth_tilt": 0.15, "defensive_tilt": -0.10, "volatility_adjustment": -0.05},
            "bull": {"growth_tilt": 0.10, "defensive_tilt": -0.05, "volatility_adjustment": 0},
            "bear": {"growth_tilt": -0.15, "defensive_tilt": 0.20, "volatility_adjustment": 0.10},
            "high_volatility": {"growth_tilt": -0.10, "defensive_tilt": 0.15, "volatility_adjustment": 0.15},
            "low_volatility": {"growth_tilt": 0.05, "defensive_tilt": -0.05, "volatility_adjustment": -0.10},
            "sideways": {"growth_tilt": 0, "defensive_tilt": 0.05, "volatility_adjustment": 0},
            "neutral": {"growth_tilt": 0, "defensive_tilt": 0, "volatility_adjustment": 0}
        }
        return adjustments.get(regime, adjustments["neutral"])

    def analyze_stock_factors(self, stock: str) -> Dict[str, float]:
        """
        AI TECHNIQUE: Multi-Factor Analysis for Stock Selection
        
        Analyzes stocks across multiple quantitative factors:
        - Momentum (6-month and 12-month returns)
        - Volatility (risk measure) 
        - Quality (consistency, dividend yield)
        - Value (P/E, P/B ratios)
        - Size (market cap effect)
        - Technical (RSI, moving averages)
        """
        try:
            # Get 2 years of data for comprehensive analysis
            ticker = yf.Ticker(stock)
            hist_data = ticker.history(period="2y")
            info = ticker.info
            
            if len(hist_data) < 200:
                return self._default_factor_scores()
            
            prices = hist_data['Close']
            volumes = hist_data['Volume']
            returns = prices.pct_change().dropna()
            
            # 1. Momentum Factors
            momentum_6m = (prices.iloc[-1] / prices.iloc[-126] - 1) if len(prices) >= 126 else 0
            momentum_12m = (prices.iloc[-1] / prices.iloc[-252] - 1) if len(prices) >= 252 else 0
            momentum_score = (momentum_6m * 0.6 + momentum_12m * 0.4) * 2  # Scale to [-2, 2]
            momentum_score = np.tanh(momentum_score)  # Normalize to [-1, 1]
            
            # 2. Volatility Factor (lower volatility = higher score for stability)
            annual_volatility = returns.std() * np.sqrt(252)
            volatility_score = max(-1, min(1, (0.20 - annual_volatility) / 0.15))  # Normalize around 20% vol
            
            # 3. Quality Factors
            # Consistency of returns (lower standard deviation = higher quality)
            monthly_returns = returns.rolling(21).sum().dropna()
            return_consistency = 1 - min(1, monthly_returns.std() * 4)
            
            # Volume consistency (more consistent volume = higher quality)
            volume_consistency = 1 - min(1, (volumes.std() / volumes.mean())) if volumes.mean() > 0 else 0
            
            quality_score = (return_consistency * 0.7 + volume_consistency * 0.3)
            quality_score = (quality_score - 0.5) * 2  # Scale to [-1, 1]
            
            # 4. Value Factors (from fundamental data)
            value_score = 0
            try:
                pe_ratio = info.get('trailingPE', None)
                pb_ratio = info.get('priceToBook', None)
                
                if pe_ratio and pe_ratio > 0:
                    # Lower P/E = higher value score
                    pe_score = max(-1, min(1, (25 - pe_ratio) / 20))
                    value_score += pe_score * 0.6
                
                if pb_ratio and pb_ratio > 0:
                    # Lower P/B = higher value score  
                    pb_score = max(-1, min(1, (3 - pb_ratio) / 2))
                    value_score += pb_score * 0.4
                    
                if not pe_ratio and not pb_ratio:
                    value_score = 0  # Neutral if no fundamental data
                    
            except:
                value_score = 0
            
            # 5. Size Factor
            try:
                market_cap = info.get('marketCap', 0)
                if market_cap > 0:
                    # Log scale for market cap, normalized
                    log_cap = np.log10(market_cap)
                    # Favor mid to large cap (9-12 on log scale)
                    size_score = max(-1, min(1, (log_cap - 8) / 4 - 0.5))
                else:
                    size_score = 0
            except:
                size_score = 0
            
            # 6. Technical Factors (RSI-like momentum)
            rsi_periods = 14
            if len(returns) >= rsi_periods:
                gains = returns.where(returns > 0, 0).rolling(rsi_periods).mean()
                losses = -returns.where(returns < 0, 0).rolling(rsi_periods).mean()
                rs = gains / (losses + 1e-10)
                rsi = 100 - (100 / (1 + rs))
                current_rsi = rsi.iloc[-1]
                # Convert RSI to score: 30-70 neutral, <30 oversold (good), >70 overbought (bad)
                technical_score = max(-1, min(1, (50 - current_rsi) / 25))
            else:
                technical_score = 0
            
            # Calculate composite score
            composite = (momentum_score * 0.25 + quality_score * 0.25 + 
                        volatility_score * 0.2 + value_score * 0.15 + 
                        size_score * 0.1 + technical_score * 0.05)
            
            return {
                "momentum": momentum_score,
                "volatility": volatility_score,  
                "quality": quality_score,
                "value": value_score,
                "size": size_score,
                "technical": technical_score,
                "composite": composite
            }
            
        except Exception as e:
            logger.warning(f"Factor analysis failed for {stock}: {e}")
            return self._default_factor_scores()

    def _default_factor_scores(self) -> Dict[str, float]:
        """Default factor scores when analysis fails."""
        return {
            "momentum": 0, "volatility": 0, "quality": 0,
            "value": 0, "size": 0, "technical": 0, "composite": 0
        }

    def optimize_for_diversification(self, stocks: List[str], target_weights: Dict[str, float]) -> Dict[str, float]:
        """
        FIXED: Correlation-Based Portfolio Optimization
        """
        try:
            # Use shorter time period to ensure we get data
            end_date = datetime.now()
            start_date = end_date - timedelta(days=180)  # 6 months instead of 1 year
            
            price_data = {}
            valid_stocks = []
            
            logger.info(f"Downloading correlation data for {len(stocks)} stocks...")
            
            for stock in stocks:
                try:
                    # Use shorter period and add timeout
                    data = yf.download(stock, start=start_date, end=end_date, 
                                    progress=False, timeout=10)['Close']
                    
                    if len(data) > 50:  # Reduced minimum data requirement
                        price_data[stock] = data
                        valid_stocks.append(stock)
                        logger.debug(f"â {stock}: {len(data)} data points")
                    else:
                        logger.warning(f"â {stock}: insufficient data ({len(data)} points)")
                        
                except Exception as e:
                    logger.warning(f"â {stock}: download failed - {str(e)[:50]}")
                    continue
            
            if len(valid_stocks) < 3:
                logger.warning(f"Only {len(valid_stocks)} stocks have data, using target weights")
                return target_weights
            
            # Create returns DataFrame
            returns_df = pd.DataFrame()
            for stock in valid_stocks:
                returns_df[stock] = price_data[stock].pct_change().dropna()
            
            # Remove any remaining NaN values and align dates
            returns_df = returns_df.dropna()
            
            if len(returns_df) < 30:  # Reduced minimum requirement
                logger.warning(f"Only {len(returns_df)} return observations, using target weights")
                return target_weights
            
            logger.info(f"â Correlation analysis with {len(valid_stocks)} stocks, {len(returns_df)} observations")
            
            # Calculate correlation matrix
            correlation_matrix = returns_df.corr()
            
            # Check for any NaN correlations
            correlation_matrix = correlation_matrix.fillna(0)
            
            # Calculate average correlation for each stock with others
            avg_correlations = {}
            for stock in valid_stocks:
                other_stocks = [s for s in valid_stocks if s != stock]
                if other_stocks:
                    corr_values = correlation_matrix.loc[stock, other_stocks]
                    # Handle any remaining NaN values
                    avg_corr = corr_values.fillna(0).mean()
                    avg_correlations[stock] = avg_corr
                    logger.debug(f"{stock}: avg correlation = {avg_corr:.3f}")
            
            # Optimize weights: reduce weights for highly correlated assets
            optimized_weights = {}
            base_weight = 1.0 / len(valid_stocks)
            
            for stock in valid_stocks:
                correlation_penalty = avg_correlations.get(stock, 0)
                
                # Reduce weight for highly correlated assets
                if correlation_penalty > 0.7:
                    weight_adjustment = -0.3
                elif correlation_penalty > 0.5:
                    weight_adjustment = -0.15
                elif correlation_penalty < 0.2:
                    weight_adjustment = 0.2  # Boost low-correlation assets
                else:
                    weight_adjustment = 0
                
                optimized_weights[stock] = max(0.05, base_weight + (base_weight * weight_adjustment))
            
            # Normalize weights
            total_weight = sum(optimized_weights.values())
            if total_weight > 0:
                optimized_weights = {k: v / total_weight for k, v in optimized_weights.items()}
            else:
                return target_weights
            
            # Blend with target allocation (70% optimized, 30% target)
            final_weights = {}
            for stock in valid_stocks:
                target_weight = target_weights.get(stock, base_weight)
                optimized_weight = optimized_weights.get(stock, base_weight)
                final_weights[stock] = 0.7 * optimized_weight + 0.3 * target_weight
            
            # Normalize final weights
            total_final = sum(final_weights.values())
            if total_final > 0:
                final_weights = {k: v / total_final for k, v in final_weights.items()}
                
                logger.info(f"â Correlation optimization successful for {len(final_weights)} stocks")
                return final_weights
            else:
                return target_weights
                
        except Exception as e:
            logger.error(f"Correlation optimization failed: {e}")
            return target_weights

    def calculate_portfolio_metrics(self, stocks: List[str], weights: Dict[str, float]) -> Dict[str, float]:
        """
        FIXED: Statistical Learning for Portfolio Risk Assessment
        """
        try:
            # Use shorter time period for more reliable data
            end_date = datetime.now()
            start_date = end_date - timedelta(days=180)  # 6 months
            
            returns_data = []
            valid_weights = []
            valid_stocks = []
            
            logger.info(f"Calculating metrics for {len(stocks)} stocks...")
            
            for stock in stocks:
                if stock in weights:
                    try:
                        data = yf.download(stock, start=start_date, end=end_date, 
                                        progress=False, timeout=10)['Close']
                        
                        if len(data) > 30:  # Reduced requirement
                            returns = data.pct_change().dropna()
                            if len(returns) > 20:
                                returns_data.append(returns)
                                valid_weights.append(weights[stock])
                                valid_stocks.append(stock)
                                logger.debug(f"â {stock}: {len(returns)} returns")
                            else:
                                logger.warning(f"â {stock}: insufficient returns data")
                        else:
                            logger.warning(f"â {stock}: insufficient price data")
                            
                    except Exception as e:
                        logger.warning(f"â {stock}: failed to get data - {str(e)[:50]}")
                        continue
            
            if len(returns_data) < 2:
                logger.warning(f"Only {len(returns_data)} stocks have data, using defaults")
                return {"expected_return": 0.08, "volatility": 0.15, "sharpe_ratio": 0.5, "portfolio_size": len(stocks)}
            
            # Align all return series to same dates
            returns_df = pd.concat(returns_data, axis=1, keys=valid_stocks)
            returns_df = returns_df.dropna()
            
            if len(returns_df) < 20:
                logger.warning(f"Only {len(returns_df)} aligned observations, using defaults")
                return {"expected_return": 0.08, "volatility": 0.15, "sharpe_ratio": 0.5, "portfolio_size": len(stocks)}
            
            # Normalize weights for valid stocks only
            valid_weights = np.array(valid_weights)
            valid_weights = valid_weights / valid_weights.sum()
            
            # Calculate portfolio returns
            portfolio_returns = (returns_df * valid_weights).sum(axis=1)
            
            # Calculate metrics
            expected_annual_return = portfolio_returns.mean() * 252
            annual_volatility = portfolio_returns.std() * np.sqrt(252)
            
            # Handle edge cases
            if annual_volatility == 0:
                sharpe_ratio = 0
            else:
                # Assume 2% risk-free rate
                risk_free_rate = 0.02
                sharpe_ratio = (expected_annual_return - risk_free_rate) / annual_volatility
            
            # Ensure reasonable bounds
            expected_annual_return = max(-0.5, min(1.0, expected_annual_return))  # -50% to +100%
            annual_volatility = max(0.01, min(2.0, annual_volatility))           # 1% to 200%
            sharpe_ratio = max(-5, min(5, sharpe_ratio))                         # -5 to +5
            
            logger.info(f"â Portfolio metrics calculated: {expected_annual_return:.1%} return, {annual_volatility:.1%} vol, {sharpe_ratio:.2f} Sharpe")
            
            return {
                "expected_return": expected_annual_return,
                "volatility": annual_volatility,
                "sharpe_ratio": sharpe_ratio,
                "portfolio_size": len(valid_stocks),
                "data_points": len(returns_df)
            }
            
        except Exception as e:
            logger.error(f"Portfolio metrics calculation failed: {e}")
            return {"expected_return": 0.08, "volatility": 0.15, "sharpe_ratio": 0.5, "portfolio_size": len(stocks)}

    def rank_stocks_by_factors(self, stocks: List[str], factor_weights: Dict[str, float] = None) -> List[Tuple[str, float]]:
        """
        AI TECHNIQUE: Multi-Criteria Decision Making
        
        Ranks stocks using weighted factor scores with customizable weights.
        Returns list of (stock, score) tuples sorted by composite score.
        """
        if factor_weights is None:
            factor_weights = {
                "momentum": 0.25, "quality": 0.25, "volatility": 0.20,
                "value": 0.15, "size": 0.10, "technical": 0.05
            }
        
        stock_scores = []
        
        for stock in stocks:
            factor_scores = self.analyze_stock_factors(stock)
            
            # Calculate weighted composite score
            composite_score = sum(
                factor_scores.get(factor, 0) * weight 
                for factor, weight in factor_weights.items()
            )
            
            stock_scores.append((stock, composite_score))
            logger.debug(f"{stock}: Composite score = {composite_score:.3f}")
        
        # Sort by score (highest first)
        stock_scores.sort(key=lambda x: x[1], reverse=True)
        
        return stock_scores

    # =====================================================================
    # ENHANCED MAIN RECOMMENDATION FUNCTION
    # =====================================================================

    def recommend_stocks(self, target_value: float, timeframe: int, risk_score: float,
                        current_investment: float = 0, monthly_contribution: float = 0) -> List[str]:
        """
        ENHANCED: AI-Powered Stock Recommendation System
        
        Uses multiple AI techniques:
        1. Market regime detection (time series analysis)
        2. Factor-based stock selection (machine learning)
        3. Correlation optimization (statistical learning)
        4. Goal achievement prediction (predictive analytics)
        5. Risk assessment (quantitative analysis)
        """
        
        try:
            logger.info(f"ð AI-ENHANCED optimization for goal: Â£{target_value:,} in {timeframe} years (risk: {risk_score})")
            
            # Step 1: AI Market Regime Detection
            market_regime = self.detect_market_regime()
            logger.info(f"ð Market regime: {market_regime['regime']} (confidence: {market_regime['confidence']:.0%})")
            
            # Step 2: Determine base risk category
            risk_category = self.risk_score_to_category(risk_score)
            
            # Step 3: Adjust risk category based on AI market analysis
            adjusted_risk_category = self._adjust_risk_for_market_regime(risk_category, market_regime)
            
            # Step 4: Get initial stock selection using existing logic
            recommended_stocks, recommendation_info = self.select_optimal_stocks(
                adjusted_risk_category, timeframe, target_value, current_investment, monthly_contribution
            )
            
            # Step 5: Expand stock universe for AI factor analysis
            expanded_universe = self._get_expanded_universe(adjusted_risk_category)
            
            # Step 6: Apply AI factor-based ranking
            factor_weights = self._get_factor_weights_for_regime(market_regime['regime'], timeframe)
            ranked_stocks = self.rank_stocks_by_factors(expanded_universe, factor_weights)
            
            # Step 7: Select top-ranked stocks (6-10 holdings for optimal diversification)
            target_portfolio_size = min(10, max(6, len(recommended_stocks)))
            top_stocks = [stock for stock, score in ranked_stocks[:target_portfolio_size]]
            
            # Step 8: Validate stock availability
            valid_stocks = self.validate_and_filter_stocks(top_stocks)
            
            if len(valid_stocks) < 4:
                logger.warning("â ï¸ AI factor selection resulted in too few stocks, using fallback")
                valid_stocks = self.validate_and_filter_stocks(recommended_stocks)
            
            # Step 9: AI correlation-based weight optimization
            initial_weights = {stock: 1.0/len(valid_stocks) for stock in valid_stocks}
            optimized_weights = self.optimize_for_diversification(valid_stocks, initial_weights)
            
            # Step 10: Calculate AI-powered portfolio metrics
            portfolio_metrics = self.calculate_portfolio_metrics(valid_stocks, optimized_weights)
            
            # Step 11: Validate goal achievability with AI analysis
            goal_assessment = self.assess_goal_feasibility(
                recommendation_info["required_return"], risk_score
            )
            
            # Log comprehensive AI results
            logger.info(f"â AI-ENHANCED portfolio created:")
            logger.info(f"   ð Expected return: {portfolio_metrics['expected_return']:.1%}")
            logger.info(f"   ð Volatility: {portfolio_metrics['volatility']:.1%}")
            logger.info(f"   â¡ Sharpe ratio: {portfolio_metrics['sharpe_ratio']:.2f}")
            logger.info(f"   ð¯ Goal feasibility: {goal_assessment['feasibility_score']:.0f}%")
            logger.info(f"   ð AI-selected stocks: {valid_stocks}")
            
            # Store AI metadata for analysis and continuous learning
            self._store_recommendation_metadata({
                "stocks": valid_stocks,
                "weights": optimized_weights,
                "market_regime": market_regime,
                "portfolio_metrics": portfolio_metrics,
                "goal_assessment": goal_assessment,
                "factor_weights": factor_weights,
                "ai_enhanced": True
            })
            
            return valid_stocks
            
        except Exception as e:
            logger.error(f"â AI-enhanced recommendation failed: {str(e)}")
            # Fallback to original method for reliability
            return self._fallback_recommendation(risk_score)

    def _adjust_risk_for_market_regime(self, risk_category: str, market_regime: Dict) -> str:
        """AI-powered risk adjustment based on market conditions."""
        regime = market_regime['regime']
        adjustments = market_regime['adjustment_factor']
        
        # Define risk category ordering
        risk_levels = ["ultra_conservative", "conservative", "moderate", 
                       "moderate_aggressive", "aggressive", "ultra_aggressive"]
        
        current_index = risk_levels.index(risk_category) if risk_category in risk_levels else 2
        
        # AI-based adjustment logic
        if regime == "bear" or regime == "high_volatility":
            # Move toward more conservative in bad markets
            adjustment = -1
        elif regime == "strong_bull" and adjustments['growth_tilt'] > 0.1:
            # Move slightly more aggressive in strong bull markets
            adjustment = 1
        else:
            adjustment = 0
        
        new_index = max(0, min(len(risk_levels) - 1, current_index + adjustment))
        adjusted_category = risk_levels[new_index]
        
        if adjusted_category != risk_category:
            logger.info(f"ð AI risk adjusted for market: {risk_category} â {adjusted_category}")
        
        return adjusted_category

    def _get_expanded_universe(self, risk_category: str) -> List[str]:
        """Get expanded stock universe for AI factor analysis."""
        # Start with base universe
        universe = self.asset_universes.get(risk_category, self.asset_universes["moderate"])
        all_stocks = []
        
        # Collect all stocks from all categories in the universe
        for category, stocks in universe.items():
            if isinstance(stocks, list):
                all_stocks.extend(stocks)
        
        # Add backup tickers for more options
        backup_stocks = self.backup_tickers.get(risk_category, self.backup_tickers["moderate"])
        all_stocks.extend(backup_stocks)
        
        # Remove duplicates while preserving order
        expanded_universe = list(dict.fromkeys(all_stocks))
        
        return expanded_universe

    def _get_factor_weights_for_regime(self, regime: str, timeframe: int) -> Dict[str, float]:
        """AI-optimized factor weights for market regime and timeframe."""
        base_weights = {
            "momentum": 0.25, "quality": 0.25, "volatility": 0.20,
            "value": 0.15, "size": 0.10, "technical": 0.05
        }
        
        # AI adjustments for market regime
        if regime in ["bear", "high_volatility"]:
            # Emphasize quality and low volatility in bad markets
            base_weights["quality"] += 0.15
            base_weights["volatility"] += 0.10
            base_weights["momentum"] -= 0.15
            base_weights["technical"] -= 0.10
        elif regime in ["strong_bull", "bull"]:
            # Emphasize momentum in good markets
            base_weights["momentum"] += 0.15
            base_weights["technical"] += 0.05
            base_weights["volatility"] -= 0.10
            base_weights["quality"] -= 0.10
        elif regime == "sideways":
            # Emphasize value in sideways markets
            base_weights["value"] += 0.15
            base_weights["momentum"] -= 0.15
        
        # AI adjustments for timeframe
        if timeframe <= 3:
            # Short timeframe: emphasize quality and low volatility
            base_weights["quality"] += 0.10
            base_weights["volatility"] += 0.10
            base_weights["momentum"] -= 0.20
        elif timeframe >= 15:
            # Long timeframe: can emphasize momentum and value
            base_weights["momentum"] += 0.10
            base_weights["value"] += 0.05
            base_weights["volatility"] -= 0.15
        
        # Normalize weights
        total_weight = sum(base_weights.values())
        normalized_weights = {k: v/total_weight for k, v in base_weights.items()}
        
        return normalized_weights

    def _store_recommendation_metadata(self, metadata: Dict) -> None:
        """Store AI recommendation metadata for analysis and continuous learning."""
        try:
            # Store for analysis and model improvement
            self._last_recommendation_metadata = metadata
            logger.debug("ð AI recommendation metadata stored for continuous learning")
        except Exception as e:
            logger.warning(f"Failed to store AI metadata: {e}")

    def _fallback_recommendation(self, risk_score: float) -> List[str]:
        """Reliable fallback when AI enhancement fails."""
        logger.warning("ð Using fallback recommendation method")
        
        if risk_score < 30:
            return ["VTI", "BND", "VEA", "VYM", "VTEB"]
        elif risk_score < 70:
            return ["VTI", "VEA", "VWO", "BND", "VNQ"]
        else:
            return ["QQQ", "VUG", "VWO", "ARKK", "VGT"]

    # =====================================================================
    # REMAINING EXISTING METHODS (unchanged for backward compatibility)
    # =====================================================================
    
    def adjust_allocation_for_goal(self, base_allocation: Dict[str, float], 
                                 required_return: float, expected_return: float, 
                                 timeframe: int) -> Dict[str, float]:
        """
        Adjust portfolio allocation to better achieve the target goal.
        
        KEY IMPROVEMENT #3: Dynamic allocation based on goal requirements
        This tilts the portfolio toward higher/lower risk assets based on what's needed
        to reach the goal within the timeframe.
        """
        allocation = base_allocation.copy()
        return_gap = required_return - expected_return
        
        # If we need higher returns than expected, tilt toward growth
        if return_gap > 0.02:  # Need 2%+ more return
            growth_boost = min(0.2, return_gap)  # Cap boost at 20%
            
            # Identify growth and conservative categories
            growth_categories = [k for k in allocation.keys() 
                               if any(word in k.lower() for word in ['growth', 'tech', 'innovation', 'crypto'])]
            conservative_categories = [k for k in allocation.keys() 
                                     if any(word in k.lower() for word in ['bond', 'dividend', 'utility'])]
            
            # Shift allocation from conservative to growth
            if growth_categories and conservative_categories:
                boost_per_growth = growth_boost / len(growth_categories)
                reduction_per_conservative = growth_boost / len(conservative_categories)
                
                for category in growth_categories:
                    allocation[category] = min(0.6, allocation[category] + boost_per_growth)
                
                for category in conservative_categories:
                    allocation[category] = max(0.05, allocation[category] - reduction_per_conservative)
        
        # If goal is easily achievable, can be more conservative
        elif return_gap < -0.02:  # We can afford 2%+ less return
            conservative_boost = min(0.15, abs(return_gap))
            
            # Shift toward more stable assets
            bond_categories = [k for k in allocation.keys() if 'bond' in k.lower()]
            if bond_categories:
                for category in bond_categories:
                    allocation[category] = min(0.5, allocation[category] + conservative_boost / len(bond_categories))
        
        # Adjust for timeframe
        if timeframe <= 3:  # Short timeframe - more conservative
            self._make_allocation_more_conservative(allocation)
        elif timeframe >= 15:  # Long timeframe - can take more risk
            self._make_allocation_more_aggressive(allocation)
        
        # Normalize to ensure allocations sum to 1.0
        total = sum(allocation.values())
        allocation = {k: v/total for k, v in allocation.items()}
        
        return allocation
    
    def _make_allocation_more_conservative(self, allocation: Dict[str, float]) -> None:
        """Shift allocation toward more conservative assets for short timeframes."""
        bond_categories = [k for k in allocation.keys() if 'bond' in k.lower()]
        risky_categories = [k for k in allocation.keys() 
                          if any(word in k.lower() for word in ['growth', 'tech', 'crypto', 'innovation'])]
        
        if bond_categories and risky_categories:
            shift_amount = 0.15  # Shift 15% toward bonds
            boost_per_bond = shift_amount / len(bond_categories)
            reduction_per_risky = shift_amount / len(risky_categories)
            
            for category in bond_categories:
                allocation[category] += boost_per_bond
            for category in risky_categories:
                allocation[category] = max(0.05, allocation[category] - reduction_per_risky)
    
    def _make_allocation_more_aggressive(self, allocation: Dict[str, float]) -> None:
        """Shift allocation toward more aggressive assets for long timeframes."""
        growth_categories = [k for k in allocation.keys() 
                           if any(word in k.lower() for word in ['growth', 'tech', 'innovation'])]
        bond_categories = [k for k in allocation.keys() if 'bond' in k.lower()]
        
        if growth_categories and bond_categories:
            shift_amount = 0.1  # Shift 10% toward growth
            boost_per_growth = shift_amount / len(growth_categories)
            reduction_per_bond = shift_amount / len(bond_categories)
            
            for category in growth_categories:
                allocation[category] += boost_per_growth
            for category in bond_categories:
                allocation[category] = max(0.05, allocation[category] - reduction_per_bond)
    
    def select_optimal_stocks(self, risk_category: str, timeframe: int, 
                            target_value: float, current_investment: float,
                            monthly_contribution: float = 0) -> Tuple[List[str], Dict[str, any]]:
        """
        Select optimal stocks to maximize chance of reaching the goal.
        
        KEY IMPROVEMENT #4: Holistic optimization approach
        This considers goal requirements, risk tolerance, and timeframe together
        to create the best possible portfolio for the user's specific situation.
        """
        
        # Step 1: Calculate what we need to achieve the goal
        required_return = self.calculate_required_return(
            target_value, current_investment, timeframe, monthly_contribution
        )
        
        # Step 2: Assess if the goal is realistic
        goal_assessment = self.assess_goal_feasibility(required_return, 
            self._category_to_risk_score(risk_category))
        
        # Step 3: Get base allocation for risk category
        if risk_category not in self.asset_universes:
            risk_category = "moderate"  # Default fallback
        
        universe = self.asset_universes[risk_category]
        base_allocation = universe["allocation"].copy()
        expected_return = universe["expected_annual_return"]
        
        # Step 4: Adjust allocation to optimize for goal achievement
        optimized_allocation = self.adjust_allocation_for_goal(
            base_allocation, required_return, expected_return, timeframe
        )
        
        # Step 5: Select specific stocks from each category
        selected_stocks = []
        for category, weight in optimized_allocation.items():
            if category in universe and weight > 0.05:  # Only include meaningful allocations
                category_stocks = universe[category]
                
                # Select number of stocks proportional to allocation weight
                num_stocks = max(1, min(3, int(len(category_stocks) * weight * 4)))
                
                # For this implementation, select the first N stocks
                # In production, you'd want more sophisticated selection
                selected_from_category = category_stocks[:num_stocks]
                selected_stocks.extend(selected_from_category)
        
        # Remove duplicates while preserving order
        selected_stocks = list(dict.fromkeys(selected_stocks))
        
        # Ensure reasonable portfolio size (5-12 holdings)
        if len(selected_stocks) < 5:
            backup_stocks = self.backup_tickers.get(risk_category, self.backup_tickers["moderate"])
            for stock in backup_stocks:
                if stock not in selected_stocks:
                    selected_stocks.append(stock)
                if len(selected_stocks) >= 5:
                    break
        elif len(selected_stocks) > 12:
            selected_stocks = selected_stocks[:12]
        
        # Prepare detailed recommendation info
        recommendation_info = {
            "goal_assessment": goal_assessment,
            "optimized_allocation": optimized_allocation,
            "expected_return": expected_return,
            "required_return": required_return,
            "risk_category": risk_category,
            "selected_stocks": selected_stocks
        }
        
        logger.info(f"Selected {len(selected_stocks)} optimized stocks for {risk_category}: {selected_stocks}")
        return selected_stocks, recommendation_info
    
    def _category_to_risk_score(self, category: str) -> float:
        """Convert risk category back to approximate risk score for calculations."""
        category_mapping = {
            "ultra_conservative": 10, "conservative": 25, "moderate": 40,
            "moderate_aggressive": 60, "aggressive": 75, "ultra_aggressive": 90
        }
        return category_mapping.get(category, 40)
    
    def validate_and_filter_stocks(self, stocks: List[str]) -> List[str]:
        """
        Validate stock symbols and filter out invalid ones.
        
        EXPLANATION: Ensures all recommended stocks are actually tradeable
        and have current market data available.
        """
        valid_stocks = []
        
        for stock in stocks:
            try:
                ticker = yf.Ticker(stock)
                info = ticker.info
                
                # Check if stock has current market price
                if info and 'regularMarketPrice' in info and info['regularMarketPrice']:
                    valid_stocks.append(stock)
                else:
                    logger.warning(f"Stock {stock} failed validation - no market price")
            except Exception as e:
                logger.warning(f"Stock {stock} failed validation: {str(e)}")
                continue
        
        return valid_stocks
    
    def train_model(self, df: Optional[pd.DataFrame] = None) -> None:
        """Train the machine learning model on top of the rule-based system."""
        
        if df is None:
            logger.info("No training data provided, generating synthetic data...")
            df = self.generate_training_data(1000)
        
        logger.info(f"Training AI model with {len(df)} samples")
        
        # Prepare features
        X = df[["target_value", "timeframe", "risk_score"]].copy()  # Use basic features for now
        
        # Create target variable (we'll predict risk category as numeric)
        risk_categories = ["ultra_conservative", "conservative", "moderate", 
                          "moderate_aggressive", "aggressive", "ultra_aggressive"]
        category_mapping = {cat: i for i, cat in enumerate(risk_categories)}
        
        y = df["risk_category"].map(category_mapping)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Train AI model
        self.model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train_scaled, y_train)
        
        # Evaluate
        y_pred = self.model.predict(X_test_scaled)
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        
        logger.info(f"AI model training complete. RÂ² Score: {r2:.3f}, MAE: {mae:.3f}")
        
        # Save model
        self.save_model()
    
    def save_model(self) -> None:
        """Save the trained AI model and scaler."""
        model_dir = os.path.join(os.path.dirname(__file__), "models")
        os.makedirs(model_dir, exist_ok=True)
        
        model_path = os.path.join(model_dir, "stock_recommender.pkl")
        scaler_path = os.path.join(model_dir, "feature_scaler.pkl")
        
        joblib.dump(self.model, model_path)
        joblib.dump(self.scaler, scaler_path)
        
        logger.info(f"AI model saved to {model_path}")
        logger.info(f"Scaler saved to {scaler_path}")
    
    def load_model(self) -> bool:
        """Load the trained AI model and scaler."""
        try:
            model_dir = os.path.join(os.path.dirname(__file__), "models")
            model_path = os.path.join(model_dir, "stock_recommender.pkl")
            scaler_path = os.path.join(model_dir, "feature_scaler.pkl")
            
            if os.path.exists(model_path) and os.path.exists(scaler_path):
                self.model = joblib.load(model_path)
                self.scaler = joblib.load(scaler_path)
                logger.info("AI model and scaler loaded successfully")
                return True
            else:
                logger.warning("AI model files not found")
                return False
        except Exception as e:
            logger.error(f"Error loading AI model: {str(e)}")
            return False
    
    def generate_training_data(self, num_samples: int = 1000) -> pd.DataFrame:
        """Generate enhanced training data for AI model including goal-achievement features."""
        data = []
        
        for _ in range(num_samples):
            target_value = np.random.lognormal(np.log(50000), 1.0)
            target_value = max(1000, min(5000000, target_value))
            
            timeframe = np.random.choice([1, 2, 3, 5, 7, 10, 15, 20, 25, 30])
            risk_score = np.random.beta(2, 2) * 100
            
            # Calculate additional features
            current_investment = np.random.uniform(0, target_value * 0.5)
            monthly = np.random.uniform(0, target_value * 0.05)
            
            required_return = self.calculate_required_return(target_value, current_investment, timeframe, monthly)
            investment_gap = target_value - (current_investment + monthly * 12 * timeframe)
            
            risk_category = self.risk_score_to_category(risk_score)
            recommended_stocks, _ = self.select_optimal_stocks(
                risk_category, timeframe, target_value, current_investment, monthly
            )
            
            data.append({
                "target_value": target_value,
                "timeframe": timeframe,
                "risk_score": risk_score,
                "required_annual_return": required_return,
                "investment_gap": investment_gap,
                "risk_category": risk_category,
                "recommended_stocks": ",".join(recommended_stocks)
            })
        
        return pd.DataFrame(data)


# =====================================================================
# GLOBAL FUNCTIONS FOR BACKWARD COMPATIBILITY
# =====================================================================

# Global instance for backward compatibility
_recommender = EnhancedStockRecommender()

def train_and_recommend(target_value: float, timeframe: int, risk_score: float) -> List[str]:
    """
    Main function for backward compatibility with existing code.
    
    NOW AI-ENHANCED: Uses machine learning and advanced analytics
    for goal-oriented optimization instead of just risk matching.
    """
    return _recommender.recommend_stocks(target_value, timeframe, risk_score)

def save_last_input_features(target_value: float, timeframe: int, risk_score: float) -> None:
    """Save input features for audit trail and AI model improvement."""
    try:
        input_df = pd.DataFrame([{
            "target_value": target_value,
            "timeframe": timeframe,
            "risk_score": risk_score,
            "timestamp": datetime.now().isoformat()
        }])
        
        features_dir = os.path.join(os.path.dirname(__file__), "data")
        os.makedirs(features_dir, exist_ok=True)
        
        input_features_path = os.path.join(features_dir, "last_input_features.csv")
        input_df.to_csv(input_features_path, index=False)
        
        logger.info(f"Input features saved for AI learning: {input_features_path}")
    except Exception as e:
        logger.warning(f"Failed to save input features: {str(e)}")

def get_backup_tickers(count: int = 5) -> List[str]:
    """Returns reliable backup tickers for fallback scenarios."""
    reliable_tickers = [
        'VTI',   # Total Stock Market ETF
        'BND',   # Total Bond Market ETF  
        'VEA',   # Developed Markets ETF
        'VWO',   # Emerging Markets ETF
        'VNQ',   # Real Estate ETF
        'QQQ',   # NASDAQ-100 ETF
        'SPY',   # S&P 500 ETF
        'VUG',   # Growth ETF
        'VYM',   # High Dividend Yield ETF
        'VGT'    # Technology Sector ETF
    ]
    return reliable_tickers[:count]


# =====================================================================
# TESTING AND VALIDATION
# =====================================================================

if __name__ == "__main__":
    # Test the AI-enhanced system with goal-oriented scenarios
    print("ð¤ Testing AI-Enhanced Stock Recommender System")
    print("=" * 60)
    
    recommender = EnhancedStockRecommender()
    
    test_cases = [
        # (target, timeframe, risk, current_investment, monthly)
        (50000, 10, 30, 5000, 200),    # Conservative long-term goal
        (100000, 5, 60, 10000, 1000),  # Aggressive shorter-term goal  
        (25000, 3, 40, 2000, 500),     # Moderate short-term goal
    ]
    
    for i, (target, timeframe, risk, current, monthly) in enumerate(test_cases, 1):
        print(f"\nð§ª Test Case {i}:")
        print(f"Goal: Â£{target:,} in {timeframe} years (Risk: {risk})")
        print(f"Starting: Â£{current:,} + Â£{monthly}/month")
        
        # Test AI-enhanced recommendations
        stocks = recommender.recommend_stocks(target, timeframe, risk, current, monthly)
        print(f"AI-Recommended: {stocks}")
        
        # Show goal analysis
        required_return = recommender.calculate_required_return(target, current, timeframe, monthly)
        print(f"Required return: {required_return:.1%} annually")
        
        # Test market regime detection
        market_regime = recommender.detect_market_regime()
        print(f"Market regime: {market_regime['regime']} ({market_regime['confidence']:.0%} confidence)")
    
    print(f"\nâ AI-Enhanced Stock Recommender System Ready!")
    print(f"ð¬ Features: Market Regime Detection, Factor Analysis, Correlation Optimization")
    print(f"ð¯ Goal-oriented optimization with machine learning capabilities")